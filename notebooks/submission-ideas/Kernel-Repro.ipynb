{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on: https://www.kaggle.com/cttsai/forked-lgbm-w-ideas-from-kernels-and-discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "import gc; gc.enable()\n",
    "from functools import partial, wraps\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from numba import jit\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions definitions\n",
    "TODO: Refactor to separate files when the structure becomes apparent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def haversine_plus(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees) from \n",
    "    #https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "    \"\"\"\n",
    "    #Convert decimal degrees to Radians:\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    lat2 = np.radians(lat2)\n",
    "\n",
    "    #Implementing Haversine Formula: \n",
    "    dlon = np.subtract(lon2, lon1)\n",
    "    dlat = np.subtract(lat2, lat1)\n",
    "\n",
    "    a = np.add(np.power(np.sin(np.divide(dlat, 2)), 2),  \n",
    "                          np.multiply(np.cos(lat1), \n",
    "                                      np.multiply(np.cos(lat2), \n",
    "                                                  np.power(np.sin(np.divide(dlon, 2)), 2))))\n",
    "    \n",
    "    haversine = np.multiply(2, np.arcsin(np.sqrt(a)))\n",
    "    return {\n",
    "        'haversine': haversine, \n",
    "        'latlon1': np.subtract(np.multiply(lon1, lat1), np.multiply(lon2, lat2)), \n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def process_flux(df):\n",
    "    flux_ratio_sq = np.power(df['flux'].values / df['flux_err'].values, 2.0)\n",
    "\n",
    "    df_flux = pd.DataFrame({\n",
    "        'flux_ratio_sq': flux_ratio_sq, \n",
    "        'flux_by_flux_ratio_sq': df['flux'].values * flux_ratio_sq,}, \n",
    "        index=df.index)\n",
    "    \n",
    "    return pd.concat([df, df_flux], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def process_flux_agg(df):\n",
    "    flux_w_mean = df['flux_by_flux_ratio_sq_sum'].values / df['flux_ratio_sq_sum'].values\n",
    "    flux_diff = df['flux_max'].values - df['flux_min'].values\n",
    "    \n",
    "    df_flux_agg = pd.DataFrame({\n",
    "        'flux_w_mean': flux_w_mean,\n",
    "        'flux_diff1': flux_diff,\n",
    "        'flux_diff2': flux_diff / df['flux_mean'].values,       \n",
    "        'flux_diff3': flux_diff /flux_w_mean,\n",
    "        }, index=df.index)\n",
    "    \n",
    "    return pd.concat([df, df_flux_agg], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(df, df_meta, aggs, fcp, n_jobs=4):\n",
    "    \"\"\"\n",
    "    Extracting Features from train set\n",
    "    Features from olivier's kernel\n",
    "    very smart and powerful feature that is generously given here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n",
    "    per passband features with tsfresh library. fft features added to capture periodicity https://www.kaggle.com/c/PLAsTiCC-2018/discussion/70346#415506\n",
    "    \"\"\"\n",
    "    df = process_flux(df)\n",
    "\n",
    "    agg_df = df.groupby('object_id').agg(aggs)\n",
    "    agg_df.columns = [ '{}_{}'.format(k, agg) for k in aggs.keys() for agg in aggs[k]]\n",
    "    agg_df = process_flux_agg(agg_df) # new feature to play with tsfresh\n",
    "\n",
    "    # Add more features with\n",
    "    agg_df_ts_flux_passband = extract_features(df, \n",
    "                                               column_id='object_id', \n",
    "                                               column_sort='mjd', \n",
    "                                               column_kind='passband', \n",
    "                                               column_value='flux', \n",
    "                                               default_fc_parameters=fcp['flux_passband'], n_jobs=n_jobs)\n",
    "\n",
    "    agg_df_ts_flux = extract_features(df, \n",
    "                                      column_id='object_id', \n",
    "                                      column_value='flux', \n",
    "                                      default_fc_parameters=fcp['flux'], n_jobs=n_jobs)\n",
    "\n",
    "    agg_df_ts_flux_by_flux_ratio_sq = extract_features(df, \n",
    "                                      column_id='object_id', \n",
    "                                      column_value='flux_by_flux_ratio_sq', \n",
    "                                      default_fc_parameters=fcp['flux_by_flux_ratio_sq'], n_jobs=n_jobs)\n",
    "\n",
    "    # Add smart feature that is suggested here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n",
    "    # dt[detected==1, mjd_diff:=max(mjd)-min(mjd), by=object_id]\n",
    "    df_det = df[df['detected']==1].copy()\n",
    "    agg_df_mjd = extract_features(df_det, \n",
    "                                  column_id='object_id', \n",
    "                                  column_value='mjd', \n",
    "                                  default_fc_parameters=fcp['mjd'], n_jobs=n_jobs)\n",
    "    agg_df_mjd['mjd_diff_det'] = agg_df_mjd['mjd__maximum'].values - agg_df_mjd['mjd__minimum'].values\n",
    "    del agg_df_mjd['mjd__maximum'], agg_df_mjd['mjd__minimum']\n",
    "    \n",
    "    agg_df_ts_flux_passband.index.rename('object_id', inplace=True) \n",
    "    agg_df_ts_flux.index.rename('object_id', inplace=True) \n",
    "    agg_df_ts_flux_by_flux_ratio_sq.index.rename('object_id', inplace=True) \n",
    "    agg_df_mjd.index.rename('object_id', inplace=True)      \n",
    "    agg_df_ts = pd.concat([agg_df, \n",
    "                           agg_df_ts_flux_passband, \n",
    "                           agg_df_ts_flux, \n",
    "                           agg_df_ts_flux_by_flux_ratio_sq, \n",
    "                           agg_df_mjd], axis=1).reset_index()\n",
    "    \n",
    "    result = agg_df_ts.merge(right=df_meta, how='left', on='object_id')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_meta(filename):\n",
    "    meta_df = pd.read_csv(filename)\n",
    "    \n",
    "    meta_dict = dict()\n",
    "    # distance\n",
    "    meta_dict.update(haversine_plus(meta_df['ra'].values, meta_df['decl'].values, \n",
    "                   meta_df['gal_l'].values, meta_df['gal_b'].values))\n",
    "    #\n",
    "    meta_dict['hostgal_photoz_certain'] = np.multiply(\n",
    "            meta_df['hostgal_photoz'].values, \n",
    "             np.exp(meta_df['hostgal_photoz_err'].values))\n",
    "    \n",
    "    meta_df = pd.concat([meta_df, pd.DataFrame(meta_dict, index=meta_df.index)], axis=1)\n",
    "    return meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_weighted_logloss(y_true, y_preds, classes, class_weights):\n",
    "    \"\"\"\n",
    "    refactor from\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weights[k] for k in sorted(class_weights.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    refactor from\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"  \n",
    "    # Taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weights = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "\n",
    "    loss = multi_weighted_logloss(y_true, y_preds, classes, class_weights)\n",
    "    return 'wloss', loss, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_multi_weighted_logloss(y_predicted, y_true, classes, class_weights):\n",
    "    loss = multi_weighted_logloss(y_true.get_label(), y_predicted, \n",
    "                                  classes, class_weights)\n",
    "    return 'wloss', loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_importances(importances_):\n",
    "    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n",
    "    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n",
    "    return importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_modeling_cross_validation(params,\n",
    "                                  full_train, \n",
    "                                  y, \n",
    "                                  classes, \n",
    "                                  class_weights, \n",
    "                                  nr_fold=5, \n",
    "                                  random_state=1):\n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "\n",
    "    # loss function\n",
    "    func_loss = partial(xgb_multi_weighted_logloss, \n",
    "                        classes=classes, \n",
    "                        class_weights=class_weights)\n",
    "\n",
    "    clfs = []\n",
    "    importances = pd.DataFrame()\n",
    "    folds = StratifiedKFold(n_splits=nr_fold, \n",
    "                            shuffle=True, \n",
    "                            random_state=random_state)\n",
    "    \n",
    "    oof_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = full_train.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = full_train.iloc[val_], y.iloc[val_]\n",
    "    \n",
    "        clf = XGBClassifier(**params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=func_loss,\n",
    "            verbose=100,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        clfs.append(clf)\n",
    "\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, ntree_limit=clf.best_ntree_limit)\n",
    "        print('no {}-fold loss: {}'.format(fold_ + 1, \n",
    "              multi_weighted_logloss(val_y, oof_preds[val_, :], \n",
    "                                     classes, class_weights)))\n",
    "    \n",
    "        imp_df = pd.DataFrame({\n",
    "                'feature': full_train.columns,\n",
    "                'gain': clf.feature_importances_,\n",
    "                'fold': [fold_ + 1] * len(full_train.columns),\n",
    "                })\n",
    "        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "    score = multi_weighted_logloss(y_true=y, y_preds=oof_preds, \n",
    "                                   classes=classes, class_weights=class_weights)\n",
    "    print('MULTI WEIGHTED LOG LOSS: {:.5f}'.format(score))\n",
    "    df_importances = save_importances(importances_=importances)\n",
    "    df_importances.to_csv('xgb_importances.csv', index=False)\n",
    "    \n",
    "    return clfs, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_modeling_cross_validation(params,\n",
    "                                   full_train, \n",
    "                                   y, \n",
    "                                   classes, \n",
    "                                   class_weights, \n",
    "                                   nr_fold=5, \n",
    "                                   random_state=1):\n",
    "\n",
    "    # Compute weights\n",
    "    w = y.value_counts()\n",
    "    weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "\n",
    "    clfs = []\n",
    "    importances = pd.DataFrame()\n",
    "    folds = StratifiedKFold(n_splits=nr_fold, \n",
    "                            shuffle=True, \n",
    "                            random_state=random_state)\n",
    "    \n",
    "    oof_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "        trn_x, trn_y = full_train.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = full_train.iloc[val_], y.iloc[val_]\n",
    "    \n",
    "        clf = LGBMClassifier(**params)\n",
    "        clf.fit(\n",
    "            trn_x, trn_y,\n",
    "            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "            eval_metric=lgbm_multi_weighted_logloss,\n",
    "            verbose=100,\n",
    "            early_stopping_rounds=50,\n",
    "            sample_weight=trn_y.map(weights)\n",
    "        )\n",
    "        clfs.append(clf)\n",
    "\n",
    "        oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "        print('no {}-fold loss: {}'.format(fold_ + 1, \n",
    "              multi_weighted_logloss(val_y, oof_preds[val_, :], \n",
    "                                     classes, class_weights)))\n",
    "    \n",
    "        imp_df = pd.DataFrame({\n",
    "                'feature': full_train.columns,\n",
    "                'gain': clf.feature_importances_,\n",
    "                'fold': [fold_ + 1] * len(full_train.columns),\n",
    "                })\n",
    "        importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "    score = multi_weighted_logloss(y_true=y, y_preds=oof_preds, \n",
    "                                   classes=classes, class_weights=class_weights)\n",
    "    print('MULTI WEIGHTED LOG LOSS: {:.5f}'.format(score))\n",
    "    df_importances = save_importances(importances_=importances)\n",
    "    df_importances.to_csv('lgbm_importances.csv', index=False)\n",
    "    \n",
    "    return clfs, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_chunk(df_, clfs_, meta_, features, featurize_configs, train_mean):\n",
    "    \n",
    "    # process all features\n",
    "    full_test = featurize(df_, meta_, \n",
    "                          featurize_configs['aggs'], \n",
    "                          featurize_configs['fcp'])\n",
    "    full_test.fillna(0, inplace=True)\n",
    "\n",
    "    # Make predictions\n",
    "    preds_ = None\n",
    "    for clf in clfs_:\n",
    "        if preds_ is None:\n",
    "            preds_ = clf.predict_proba(full_test[features])\n",
    "        else:\n",
    "            preds_ += clf.predict_proba(full_test[features])\n",
    "            \n",
    "    preds_ = preds_ / len(clfs_)\n",
    "\n",
    "    # Compute preds_99 as the proba of class not being any of the others\n",
    "    # preds_99 = 0.1 gives 1.769\n",
    "    preds_99 = np.ones(preds_.shape[0])\n",
    "    for i in range(preds_.shape[1]):\n",
    "        preds_99 *= (1 - preds_[:, i])\n",
    "\n",
    "    # Create DataFrame from predictions\n",
    "    preds_df_ = pd.DataFrame(preds_, \n",
    "                             columns=['class_{}'.format(s) for s in clfs_[0].classes_])\n",
    "    preds_df_['object_id'] = full_test['object_id']\n",
    "    preds_df_['class_99'] = 0.14 * preds_99 / np.mean(preds_99)\n",
    "    return preds_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test(clfs, \n",
    "                 features, \n",
    "                 featurize_configs, \n",
    "                 train_mean,\n",
    "                 filename='predictions.csv',\n",
    "                 chunks=5000000):\n",
    "    start = time.time()\n",
    "\n",
    "    meta_test = process_meta('../data/raw/test_set_metadata.csv')\n",
    "    # meta_test.set_index('object_id',inplace=True)\n",
    "\n",
    "    remain_df = None\n",
    "    for i_c, df in enumerate(pd.read_csv('../data/raw/test_set.csv', chunksize=chunks, iterator=True)):\n",
    "        # Check object_ids\n",
    "        # I believe np.unique keeps the order of group_ids as they appear in the file\n",
    "        unique_ids = np.unique(df['object_id'])\n",
    "        \n",
    "        new_remain_df = df.loc[df['object_id'] == unique_ids[-1]].copy()\n",
    "        if remain_df is None:\n",
    "            df = df.loc[df['object_id'].isin(unique_ids[:-1])]\n",
    "        else:\n",
    "            df = pd.concat([remain_df, df.loc[df['object_id'].isin(unique_ids[:-1])]], axis=0)\n",
    "        # Create remaining samples df\n",
    "        remain_df = new_remain_df\n",
    "        \n",
    "        preds_df = predict_chunk(df_=df,\n",
    "                                 clfs_=clfs,\n",
    "                                 meta_=meta_test,\n",
    "                                 features=features,\n",
    "                                 featurize_configs=featurize_configs,\n",
    "                                 train_mean=train_mean)\n",
    "    \n",
    "        if i_c == 0:\n",
    "            preds_df.to_csv(filename, header=True, mode='a', index=False)\n",
    "        else:\n",
    "            preds_df.to_csv(filename, header=False, mode='a', index=False)\n",
    "    \n",
    "        del preds_df\n",
    "        gc.collect()\n",
    "        print('{:15d} done in {:5.1f} minutes' .format(\n",
    "                chunks * (i_c + 1), (time.time() - start) / 60), flush=True)\n",
    "        \n",
    "    # Compute last object in remain_df\n",
    "    preds_df = predict_chunk(df_=remain_df,\n",
    "                             clfs_=clfs,\n",
    "                             meta_=meta_test,\n",
    "                             features=features,\n",
    "                             featurize_configs=featurize_configs,\n",
    "                             train_mean=train_mean)\n",
    "        \n",
    "    preds_df.to_csv(filename, header=False, mode='a', index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs = {\n",
    "    'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "    'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "    'detected': ['mean'],\n",
    "    'flux_ratio_sq':['sum', 'skew'],\n",
    "    'flux_by_flux_ratio_sq':['sum','skew'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcp = {\n",
    "    'flux': {\n",
    "        'longest_strike_above_mean': None,\n",
    "        'longest_strike_below_mean': None,\n",
    "        'mean_change': None,\n",
    "        'mean_abs_change': None,\n",
    "        'length': None,\n",
    "    },\n",
    "\n",
    "    'flux_by_flux_ratio_sq': {\n",
    "        'longest_strike_above_mean': None,\n",
    "        'longest_strike_below_mean': None,       \n",
    "    },\n",
    "\n",
    "    'flux_passband': {\n",
    "        'fft_coefficient': [\n",
    "                {'coeff': 0, 'attr': 'abs'}, \n",
    "                {'coeff': 1, 'attr': 'abs'}\n",
    "            ],\n",
    "        'kurtosis' : None, \n",
    "        'skewness' : None,\n",
    "    },\n",
    "\n",
    "    'mjd': {\n",
    "        'maximum': None, \n",
    "        'minimum': None,\n",
    "        'mean_change': None,\n",
    "        'mean_abs_change': None,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'device': 'cpu', \n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 14, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'n_jobs': 16, \n",
    "    'max_depth': 7, \n",
    "    'n_estimators': 1024, \n",
    "    'subsample_freq': 2, \n",
    "    'subsample_for_bin': 5000, \n",
    "    'min_data_per_group': 100, \n",
    "    'max_cat_to_onehot': 4, \n",
    "    'cat_l2': 1.0, \n",
    "    'cat_smooth': 59.5, \n",
    "    'max_cat_threshold': 32, \n",
    "    'metric_freq': 10, \n",
    "    'verbosity': -1, \n",
    "    'metric': 'multi_logloss', \n",
    "    'xgboost_dart_mode': False, \n",
    "    'uniform_drop': False, \n",
    "    'colsample_bytree': 0.5, \n",
    "    'drop_rate': 0.173, \n",
    "    'learning_rate': 0.0267, \n",
    "    'max_drop': 5, \n",
    "    'min_child_samples': 10, \n",
    "    'min_child_weight': 100.0, \n",
    "    'min_split_gain': 0.1, \n",
    "    'num_leaves': 7, \n",
    "    'reg_alpha': 0.1, \n",
    "    'reg_lambda': 0.00023, \n",
    "    'skip_drop': 0.44, \n",
    "    'subsample': 0.75\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.27 s, sys: 144 ms, total: 2.41 s\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "meta_train = process_meta('../data/raw/training_set_metadata.csv')\n",
    "train = pd.read_csv('../data/raw/training_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:08<00:00,  2.43it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 10.17it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 16.03it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 15.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 1.82 s, total: 1min 2s\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_train = featurize(train, meta_train, aggs, fcp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'target' in full_train:\n",
    "    y = full_train['target']\n",
    "    del full_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes : 14, [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
      "{6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n"
     ]
    }
   ],
   "source": [
    "# Taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "# https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "# with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "classes = sorted(y.unique())\n",
    "class_weights = {c: 1 for c in classes}\n",
    "class_weights.update({c:2 for c in [64, 15]})\n",
    "print('Unique classes : {}, {}'.format(len(classes), classes))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check: classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "#sanity check: class_weights = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "#if len(np.unique(y_true)) > 14:\n",
    "#    classes.append(99)\n",
    "#    class_weights[99] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'object_id' in full_train:\n",
    "    oof_df = full_train[['object_id']]\n",
    "    del full_train['object_id'] \n",
    "    #del full_train['distmod'] \n",
    "    del full_train['hostgal_specz']\n",
    "    del full_train['ra'], full_train['decl'], full_train['gal_l'], full_train['gal_b']\n",
    "    del full_train['ddf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   count          mean  \\\n",
      "flux_min                                          7848.0 -5.817377e+02   \n",
      "flux_max                                          7848.0  1.224459e+03   \n",
      "flux_mean                                         7848.0  3.351244e+01   \n",
      "flux_median                                       7848.0 -1.317095e+01   \n",
      "flux_std                                          7848.0  2.775506e+02   \n",
      "flux_skew                                         7848.0  2.158618e+00   \n",
      "flux_err_min                                      7848.0  2.936255e+00   \n",
      "flux_err_max                                      7848.0  4.104654e+02   \n",
      "flux_err_mean                                     7848.0  3.303208e+01   \n",
      "flux_err_median                                   7848.0  1.248738e+01   \n",
      "flux_err_std                                      7848.0  6.978792e+01   \n",
      "flux_err_skew                                     7848.0  2.483038e+00   \n",
      "detected_mean                                     7848.0  1.573900e-01   \n",
      "flux_ratio_sq_sum                                 7848.0  1.100639e+05   \n",
      "flux_ratio_sq_skew                                7848.0  6.669395e+00   \n",
      "flux_by_flux_ratio_sq_sum                         7848.0  4.718638e+07   \n",
      "flux_by_flux_ratio_sq_skew                        7848.0  5.916174e+00   \n",
      "flux_w_mean                                       7848.0  1.999271e+02   \n",
      "flux_diff1                                        7848.0  1.806196e+03   \n",
      "flux_diff2                                        7848.0  2.389844e+01   \n",
      "flux_diff3                                        7848.0  2.114325e+00   \n",
      "0__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  8.686732e+02   \n",
      "0__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  1.083683e+03   \n",
      "0__kurtosis                                       7841.0  2.931102e+00   \n",
      "0__skewness                                       7847.0  6.589448e-01   \n",
      "1__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  1.534345e+03   \n",
      "1__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  9.258924e+02   \n",
      "1__kurtosis                                       7845.0  5.833965e+00   \n",
      "1__skewness                                       7848.0  1.343296e+00   \n",
      "2__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  2.706631e+03   \n",
      "2__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  1.587983e+03   \n",
      "2__kurtosis                                       7848.0  7.780067e+00   \n",
      "2__skewness                                       7848.0  1.764522e+00   \n",
      "3__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  2.855174e+03   \n",
      "3__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  1.597846e+03   \n",
      "3__kurtosis                                       7848.0  6.708265e+00   \n",
      "3__skewness                                       7848.0  1.611161e+00   \n",
      "4__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  4.109145e+03   \n",
      "4__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  2.023273e+03   \n",
      "4__kurtosis                                       7848.0  6.259162e+00   \n",
      "4__skewness                                       7848.0  1.477267e+00   \n",
      "5__fft_coefficient__coeff_0__attr_\"abs\"           7848.0  5.195574e+03   \n",
      "5__fft_coefficient__coeff_1__attr_\"abs\"           7848.0  2.436035e+03   \n",
      "5__kurtosis                                       7848.0  4.116877e+00   \n",
      "5__skewness                                       7848.0  9.237385e-01   \n",
      "flux__length                                      7848.0  1.811551e+02   \n",
      "flux__longest_strike_above_mean                   7848.0  1.418629e+01   \n",
      "flux__longest_strike_below_mean                   7848.0  3.506651e+01   \n",
      "flux__mean_abs_change                             7848.0  1.588398e+02   \n",
      "flux__mean_change                                 7848.0  3.320639e-01   \n",
      "flux_by_flux_ratio_sq__longest_strike_above_mean  7848.0  1.057250e+01   \n",
      "flux_by_flux_ratio_sq__longest_strike_below_mean  7848.0  9.090813e+01   \n",
      "mjd__mean_abs_change                              7848.0  4.411311e+01   \n",
      "mjd__mean_change                                  7848.0  4.411311e+01   \n",
      "mjd_diff_det                                      7848.0  2.838325e+02   \n",
      "hostgal_photoz                                    7848.0  3.578845e-01   \n",
      "hostgal_photoz_err                                7848.0  1.556947e-01   \n",
      "distmod                                           5523.0  4.126396e+01   \n",
      "mwebv                                             7848.0  8.195260e-02   \n",
      "haversine                                         7848.0  1.493521e+00   \n",
      "latlon1                                           7848.0 -3.876255e-01   \n",
      "hostgal_photoz_certain                            7848.0  5.785227e-01   \n",
      "\n",
      "                                                           std           min  \\\n",
      "flux_min                                          1.342036e+04 -1.149388e+06   \n",
      "flux_max                                          2.867330e+04 -7.517203e+02   \n",
      "flux_mean                                         1.827033e+03 -4.783272e+04   \n",
      "flux_median                                       8.179198e+02 -4.336586e+04   \n",
      "flux_std                                          3.890649e+03  2.687631e+00   \n",
      "flux_skew                                         3.028822e+00 -1.281353e+01   \n",
      "flux_err_min                                      7.696597e+00  4.637530e-01   \n",
      "flux_err_max                                      2.538694e+04  9.063369e+00   \n",
      "flux_err_mean                                     1.199017e+03  2.198749e+00   \n",
      "flux_err_median                                   4.874503e+01  1.754588e+00   \n",
      "flux_err_std                                      4.112916e+03  9.761724e-01   \n",
      "flux_err_skew                                     2.966362e+00 -2.042322e+00   \n",
      "detected_mean                                     2.233782e-01  5.681818e-03   \n",
      "flux_ratio_sq_sum                                 5.943836e+05  1.269791e+02   \n",
      "flux_ratio_sq_skew                                3.110736e+00 -1.239108e+00   \n",
      "flux_by_flux_ratio_sq_sum                         3.074160e+09 -1.153257e+11   \n",
      "flux_by_flux_ratio_sq_skew                        5.626113e+00 -1.840455e+01   \n",
      "flux_w_mean                                       2.658033e+03 -6.990388e+04   \n",
      "flux_diff1                                        4.160635e+04  2.126078e+01   \n",
      "flux_diff2                                        1.436849e+03 -6.647960e+04   \n",
      "flux_diff3                                        6.559672e+01 -3.335919e+03   \n",
      "0__fft_coefficient__coeff_0__attr_\"abs\"           2.558435e+04  6.706000e-03   \n",
      "0__fft_coefficient__coeff_1__attr_\"abs\"           5.287930e+04  1.739167e-01   \n",
      "0__kurtosis                                       7.578568e+00 -3.203161e+00   \n",
      "0__skewness                                       1.560550e+00 -7.756287e+00   \n",
      "1__fft_coefficient__coeff_0__attr_\"abs\"           1.159396e+04  1.081000e-03   \n",
      "1__fft_coefficient__coeff_1__attr_\"abs\"           7.143211e+03  1.861472e-01   \n",
      "1__kurtosis                                       8.837887e+00 -4.291602e+00   \n",
      "1__skewness                                       1.977723e+00 -7.384496e+00   \n",
      "2__fft_coefficient__coeff_0__attr_\"abs\"           4.382792e+04  1.954300e-02   \n",
      "2__fft_coefficient__coeff_1__attr_\"abs\"           2.822033e+04  2.367083e-01   \n",
      "2__kurtosis                                       9.619066e+00 -2.425456e+00   \n",
      "2__skewness                                       2.131314e+00 -7.614107e+00   \n",
      "3__fft_coefficient__coeff_0__attr_\"abs\"           4.222836e+04  1.745100e-02   \n",
      "3__fft_coefficient__coeff_1__attr_\"abs\"           2.554205e+04  1.149140e+00   \n",
      "3__kurtosis                                       8.775497e+00 -2.145722e+00   \n",
      "3__skewness                                       1.986927e+00 -7.445896e+00   \n",
      "4__fft_coefficient__coeff_0__attr_\"abs\"           5.735607e+04  4.180100e-02   \n",
      "4__fft_coefficient__coeff_1__attr_\"abs\"           3.189314e+04  2.536212e+00   \n",
      "4__kurtosis                                       8.552450e+00 -1.945264e+00   \n",
      "4__skewness                                       1.905686e+00 -7.339672e+00   \n",
      "5__fft_coefficient__coeff_0__attr_\"abs\"           8.173287e+04  8.507800e-02   \n",
      "5__fft_coefficient__coeff_1__attr_\"abs\"           3.686801e+04  4.187342e+00   \n",
      "5__kurtosis                                       7.396436e+00 -2.206612e+00   \n",
      "5__skewness                                       1.630383e+00 -7.184298e+00   \n",
      "flux__length                                      9.175221e+01  4.700000e+01   \n",
      "flux__longest_strike_above_mean                   1.482250e+01  1.000000e+00   \n",
      "flux__longest_strike_below_mean                   4.844920e+01  1.000000e+00   \n",
      "flux__mean_abs_change                             1.700030e+03  2.381600e+00   \n",
      "flux__mean_change                                 2.748721e+01 -4.387450e+02   \n",
      "flux_by_flux_ratio_sq__longest_strike_above_mean  1.941361e+01  1.000000e+00   \n",
      "flux_by_flux_ratio_sq__longest_strike_below_mean  7.223568e+01  1.000000e+00   \n",
      "mjd__mean_abs_change                              1.081050e+02  8.700000e-03   \n",
      "mjd__mean_change                                  1.081050e+02  8.700000e-03   \n",
      "mjd_diff_det                                      3.480104e+02  1.090000e-02   \n",
      "hostgal_photoz                                    5.455516e-01  0.000000e+00   \n",
      "hostgal_photoz_err                                3.003674e-01  0.000000e+00   \n",
      "distmod                                           2.262711e+00  3.199610e+01   \n",
      "mwebv                                             1.505977e-01  3.000000e-03   \n",
      "haversine                                         5.827418e-01  4.282284e-02   \n",
      "latlon1                                           3.157551e+00 -6.121150e+00   \n",
      "hostgal_photoz_certain                            1.329522e+00  0.000000e+00   \n",
      "\n",
      "                                                           25%            50%  \\\n",
      "flux_min                                            -97.748140     -63.014893   \n",
      "flux_max                                             92.712827     166.902748   \n",
      "flux_mean                                             1.952291       7.054367   \n",
      "flux_median                                           0.233296       0.993680   \n",
      "flux_std                                             22.534987      35.915890   \n",
      "flux_skew                                             0.656051       2.149175   \n",
      "flux_err_min                                          1.046307       1.516784   \n",
      "flux_err_max                                         45.324589      54.928312   \n",
      "flux_err_mean                                         4.421238      12.828899   \n",
      "flux_err_median                                       3.257738       9.393351   \n",
      "flux_err_std                                          4.231706      10.814982   \n",
      "flux_err_skew                                         1.213292       1.499814   \n",
      "detected_mean                                         0.029814       0.071429   \n",
      "flux_ratio_sq_sum                                  1163.388509    4659.903047   \n",
      "flux_ratio_sq_skew                                    4.600050       6.557691   \n",
      "flux_by_flux_ratio_sq_sum                         18387.658261  245633.617776   \n",
      "flux_by_flux_ratio_sq_skew                            4.371290       7.062250   \n",
      "flux_w_mean                                          25.136668      83.231919   \n",
      "flux_diff1                                          160.170960     255.436458   \n",
      "flux_diff2                                           12.395337      22.569265   \n",
      "flux_diff3                                            1.494130       1.986227   \n",
      "0__fft_coefficient__coeff_0__attr_\"abs\"              18.305239      48.102023   \n",
      "0__fft_coefficient__coeff_1__attr_\"abs\"              23.458444      46.555761   \n",
      "0__kurtosis                                          -0.376250       0.634380   \n",
      "0__skewness                                          -0.246774       0.316416   \n",
      "1__fft_coefficient__coeff_0__attr_\"abs\"              17.174671      79.951323   \n",
      "1__fft_coefficient__coeff_1__attr_\"abs\"              15.636640      64.729119   \n",
      "1__kurtosis                                           0.105205       2.999472   \n",
      "1__skewness                                           0.038536       1.276447   \n",
      "2__fft_coefficient__coeff_0__attr_\"abs\"              76.046479     246.346179   \n",
      "2__fft_coefficient__coeff_1__attr_\"abs\"              63.231898     198.328051   \n",
      "2__kurtosis                                           0.802989       4.821736   \n",
      "2__skewness                                           0.427805       1.947666   \n",
      "3__fft_coefficient__coeff_0__attr_\"abs\"              93.025052     275.749970   \n",
      "3__fft_coefficient__coeff_1__attr_\"abs\"              79.165062     222.339731   \n",
      "3__kurtosis                                           0.645124       3.949253   \n",
      "3__skewness                                           0.357315       1.817056   \n",
      "4__fft_coefficient__coeff_0__attr_\"abs\"             144.869882     359.393183   \n",
      "4__fft_coefficient__coeff_1__attr_\"abs\"             129.255762     301.527388   \n",
      "4__kurtosis                                           0.641737       3.415452   \n",
      "4__skewness                                           0.297040       1.609730   \n",
      "5__fft_coefficient__coeff_0__attr_\"abs\"             149.471882     367.008601   \n",
      "5__fft_coefficient__coeff_1__attr_\"abs\"             166.007396     330.686203   \n",
      "5__kurtosis                                           0.163575       1.504293   \n",
      "5__skewness                                           0.013215       0.718874   \n",
      "flux__length                                        122.000000     136.000000   \n",
      "flux__longest_strike_above_mean                       7.000000      10.000000   \n",
      "flux__longest_strike_below_mean                       9.000000      16.000000   \n",
      "flux__mean_abs_change                                15.011102      20.554584   \n",
      "flux__mean_change                                    -0.101453       0.001611   \n",
      "flux_by_flux_ratio_sq__longest_strike_above_mean      3.000000       5.000000   \n",
      "flux_by_flux_ratio_sq__longest_strike_below_mean     44.000000      79.500000   \n",
      "mjd__mean_abs_change                                  4.218475       7.842496   \n",
      "mjd__mean_change                                      4.218475       7.842496   \n",
      "mjd_diff_det                                         40.882500      95.774050   \n",
      "hostgal_photoz                                        0.000000       0.210300   \n",
      "hostgal_photoz_err                                    0.000000       0.018000   \n",
      "distmod                                              39.845250      41.167900   \n",
      "mwebv                                                 0.018000       0.032000   \n",
      "haversine                                             1.164021       1.690880   \n",
      "latlon1                                              -2.941054      -0.987176   \n",
      "hostgal_photoz_certain                                0.000000       0.236039   \n",
      "\n",
      "                                                           75%           max  \n",
      "flux_min                                         -3.720073e+01  5.109941e+02  \n",
      "flux_max                                          3.650707e+02  2.432809e+06  \n",
      "flux_mean                                         1.746234e+01  1.403881e+05  \n",
      "flux_median                                       2.535580e+00  3.087974e+04  \n",
      "flux_std                                          7.584766e+01  2.796894e+05  \n",
      "flux_skew                                         3.441721e+00  1.849729e+01  \n",
      "flux_err_min                                      2.166468e+00  3.276297e+02  \n",
      "flux_err_max                                      6.349475e+01  2.234069e+06  \n",
      "flux_err_mean                                     1.551099e+01  1.047130e+05  \n",
      "flux_err_median                                   1.195244e+01  3.853605e+03  \n",
      "flux_err_std                                      1.239876e+01  3.605993e+05  \n",
      "flux_err_skew                                     1.803128e+00  1.378938e+01  \n",
      "detected_mean                                     1.666667e-01  1.000000e+00  \n",
      "flux_ratio_sq_sum                                 2.592588e+04  1.086076e+07  \n",
      "flux_ratio_sq_skew                                8.634171e+00  1.873359e+01  \n",
      "flux_by_flux_ratio_sq_sum                         2.979438e+06  1.570616e+11  \n",
      "flux_by_flux_ratio_sq_skew                        9.486608e+00  1.876046e+01  \n",
      "flux_w_mean                                       2.013387e+02  1.793980e+05  \n",
      "flux_diff1                                        5.078297e+02  3.582197e+06  \n",
      "flux_diff2                                        4.301897e+01  6.077639e+04  \n",
      "flux_diff3                                        3.085920e+00  2.836464e+03  \n",
      "0__fft_coefficient__coeff_0__attr_\"abs\"           1.675728e+02  2.230438e+06  \n",
      "0__fft_coefficient__coeff_1__attr_\"abs\"           1.393153e+02  4.631342e+06  \n",
      "0__kurtosis                                       2.905992e+00  7.155293e+01  \n",
      "0__skewness                                       1.276045e+00  8.446201e+00  \n",
      "1__fft_coefficient__coeff_0__attr_\"abs\"           3.871591e+02  6.352250e+05  \n",
      "1__fft_coefficient__coeff_1__attr_\"abs\"           2.915286e+02  4.443702e+05  \n",
      "1__kurtosis                                       8.990190e+00  5.777428e+01  \n",
      "1__skewness                                       2.842803e+00  7.593994e+00  \n",
      "2__fft_coefficient__coeff_0__attr_\"abs\"           8.201768e+02  3.741323e+06  \n",
      "2__fft_coefficient__coeff_1__attr_\"abs\"           6.110753e+02  2.428124e+06  \n",
      "2__kurtosis                                       1.183584e+01  5.798279e+01  \n",
      "2__skewness                                       3.143396e+00  7.609116e+00  \n",
      "3__fft_coefficient__coeff_0__attr_\"abs\"           8.380184e+02  3.265140e+06  \n",
      "3__fft_coefficient__coeff_1__attr_\"abs\"           6.382118e+02  2.145546e+06  \n",
      "3__kurtosis                                       9.605804e+00  5.771809e+01  \n",
      "3__skewness                                       2.848140e+00  7.588468e+00  \n",
      "4__fft_coefficient__coeff_0__attr_\"abs\"           9.435420e+02  3.667709e+06  \n",
      "4__fft_coefficient__coeff_1__attr_\"abs\"           7.284919e+02  2.625974e+06  \n",
      "4__kurtosis                                       8.633458e+00  5.792441e+01  \n",
      "4__skewness                                       2.623668e+00  7.608456e+00  \n",
      "5__fft_coefficient__coeff_0__attr_\"abs\"           9.050253e+02  4.046475e+06  \n",
      "5__fft_coefficient__coeff_1__attr_\"abs\"           7.192184e+02  2.876665e+06  \n",
      "5__kurtosis                                       4.586256e+00  5.610575e+01  \n",
      "5__skewness                                       1.783578e+00  7.466092e+00  \n",
      "flux__length                                      2.550000e+02  3.520000e+02  \n",
      "flux__longest_strike_above_mean                   1.600000e+01  2.310000e+02  \n",
      "flux__longest_strike_below_mean                   3.525000e+01  3.310000e+02  \n",
      "flux__mean_abs_change                             3.197212e+01  1.172606e+05  \n",
      "flux__mean_change                                 1.184764e-01  2.246845e+03  \n",
      "flux_by_flux_ratio_sq__longest_strike_above_mean  8.000000e+00  2.830000e+02  \n",
      "flux_by_flux_ratio_sq__longest_strike_below_mean  1.140000e+02  3.460000e+02  \n",
      "mjd__mean_abs_change                              1.900680e+01  1.071030e+03  \n",
      "mjd__mean_change                                  1.900680e+01  1.071030e+03  \n",
      "mjd_diff_det                                      4.740591e+02  1.092845e+03  \n",
      "hostgal_photoz                                    4.312000e-01  2.999400e+00  \n",
      "hostgal_photoz_err                                1.223750e-01  1.734800e+00  \n",
      "distmod                                           4.239855e+01  4.702560e+01  \n",
      "mwebv                                             7.600000e-02  2.747000e+00  \n",
      "haversine                                         1.978942e+00  2.119827e+00  \n",
      "latlon1                                           2.311904e+00  8.805560e+00  \n",
      "hostgal_photoz_certain                            5.619134e-01  1.192976e+01  \n",
      "CPU times: user 260 ms, sys: 0 ns, total: 260 ms\n",
      "Wall time: 256 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_mean = full_train.mean(axis=0)\n",
    "#train_mean.to_hdf('train_data.hdf5', 'data')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "print(full_train.describe().T)\n",
    "#import pdb; pdb.set_trace()\n",
    "full_train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_func = partial(\n",
    "    lgbm_modeling_cross_validation, \n",
    "    full_train=full_train, \n",
    "    y=y, \n",
    "    classes=classes, \n",
    "    class_weights=class_weights, \n",
    "    nr_fold=5, \n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.74868\ttraining's wloss: 0.740698\tvalid_1's multi_logloss: 1.09916\tvalid_1's wloss: 0.933424\n",
      "[200]\ttraining's multi_logloss: 0.490693\ttraining's wloss: 0.480231\tvalid_1's multi_logloss: 0.877408\tvalid_1's wloss: 0.732898\n",
      "[300]\ttraining's multi_logloss: 0.388082\ttraining's wloss: 0.37642\tvalid_1's multi_logloss: 0.798258\tvalid_1's wloss: 0.675027\n",
      "[400]\ttraining's multi_logloss: 0.328428\ttraining's wloss: 0.316807\tvalid_1's multi_logloss: 0.75792\tvalid_1's wloss: 0.660313\n",
      "[500]\ttraining's multi_logloss: 0.286033\ttraining's wloss: 0.274597\tvalid_1's multi_logloss: 0.730866\tvalid_1's wloss: 0.654883\n",
      "[600]\ttraining's multi_logloss: 0.25219\ttraining's wloss: 0.241366\tvalid_1's multi_logloss: 0.710008\tvalid_1's wloss: 0.65427\n",
      "Early stopping, best iteration is:\n",
      "[569]\ttraining's multi_logloss: 0.262032\ttraining's wloss: 0.251051\tvalid_1's multi_logloss: 0.71524\tvalid_1's wloss: 0.653658\n",
      "no 1-fold loss: 0.6536578541293255\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.749094\ttraining's wloss: 0.743051\tvalid_1's multi_logloss: 1.09436\tvalid_1's wloss: 0.94413\n",
      "[200]\ttraining's multi_logloss: 0.491299\ttraining's wloss: 0.48227\tvalid_1's multi_logloss: 0.874005\tvalid_1's wloss: 0.758287\n",
      "[300]\ttraining's multi_logloss: 0.388304\ttraining's wloss: 0.378015\tvalid_1's multi_logloss: 0.795556\tvalid_1's wloss: 0.722622\n",
      "[400]\ttraining's multi_logloss: 0.328112\ttraining's wloss: 0.317457\tvalid_1's multi_logloss: 0.759415\tvalid_1's wloss: 0.717485\n",
      "[500]\ttraining's multi_logloss: 0.284628\ttraining's wloss: 0.274232\tvalid_1's multi_logloss: 0.732422\tvalid_1's wloss: 0.713619\n",
      "Early stopping, best iteration is:\n",
      "[508]\ttraining's multi_logloss: 0.281749\ttraining's wloss: 0.271337\tvalid_1's multi_logloss: 0.730829\tvalid_1's wloss: 0.713173\n",
      "no 2-fold loss: 0.7131732277327462\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.748526\ttraining's wloss: 0.741165\tvalid_1's multi_logloss: 1.10347\tvalid_1's wloss: 0.914159\n",
      "[200]\ttraining's multi_logloss: 0.49039\ttraining's wloss: 0.480827\tvalid_1's multi_logloss: 0.878738\tvalid_1's wloss: 0.702191\n",
      "[300]\ttraining's multi_logloss: 0.387559\ttraining's wloss: 0.377106\tvalid_1's multi_logloss: 0.794571\tvalid_1's wloss: 0.641854\n",
      "[400]\ttraining's multi_logloss: 0.327928\ttraining's wloss: 0.317408\tvalid_1's multi_logloss: 0.755825\tvalid_1's wloss: 0.627763\n",
      "[500]\ttraining's multi_logloss: 0.284935\ttraining's wloss: 0.274715\tvalid_1's multi_logloss: 0.728925\tvalid_1's wloss: 0.623473\n",
      "Early stopping, best iteration is:\n",
      "[512]\ttraining's multi_logloss: 0.280438\ttraining's wloss: 0.270335\tvalid_1's multi_logloss: 0.726398\tvalid_1's wloss: 0.62216\n",
      "no 3-fold loss: 0.6221595553235412\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.754305\ttraining's wloss: 0.746333\tvalid_1's multi_logloss: 1.09493\tvalid_1's wloss: 0.902631\n",
      "[200]\ttraining's multi_logloss: 0.494055\ttraining's wloss: 0.483841\tvalid_1's multi_logloss: 0.864258\tvalid_1's wloss: 0.694883\n",
      "[300]\ttraining's multi_logloss: 0.390148\ttraining's wloss: 0.379152\tvalid_1's multi_logloss: 0.780626\tvalid_1's wloss: 0.644145\n",
      "[400]\ttraining's multi_logloss: 0.329427\ttraining's wloss: 0.318641\tvalid_1's multi_logloss: 0.740012\tvalid_1's wloss: 0.635392\n",
      "[500]\ttraining's multi_logloss: 0.286146\ttraining's wloss: 0.275526\tvalid_1's multi_logloss: 0.711273\tvalid_1's wloss: 0.630027\n",
      "Early stopping, best iteration is:\n",
      "[498]\ttraining's multi_logloss: 0.28685\ttraining's wloss: 0.276238\tvalid_1's multi_logloss: 0.711927\tvalid_1's wloss: 0.629928\n",
      "no 4-fold loss: 0.6299280992785089\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.746666\ttraining's wloss: 0.737993\tvalid_1's multi_logloss: 1.09893\tvalid_1's wloss: 0.934513\n",
      "[200]\ttraining's multi_logloss: 0.487715\ttraining's wloss: 0.4777\tvalid_1's multi_logloss: 0.865879\tvalid_1's wloss: 0.731042\n",
      "[300]\ttraining's multi_logloss: 0.385948\ttraining's wloss: 0.375434\tvalid_1's multi_logloss: 0.785019\tvalid_1's wloss: 0.680361\n",
      "[400]\ttraining's multi_logloss: 0.328461\ttraining's wloss: 0.317623\tvalid_1's multi_logloss: 0.741048\tvalid_1's wloss: 0.664608\n",
      "[500]\ttraining's multi_logloss: 0.286093\ttraining's wloss: 0.275458\tvalid_1's multi_logloss: 0.710681\tvalid_1's wloss: 0.658272\n",
      "[600]\ttraining's multi_logloss: 0.252826\ttraining's wloss: 0.242616\tvalid_1's multi_logloss: 0.687072\tvalid_1's wloss: 0.656688\n",
      "Early stopping, best iteration is:\n",
      "[608]\ttraining's multi_logloss: 0.250407\ttraining's wloss: 0.240216\tvalid_1's multi_logloss: 0.685245\tvalid_1's wloss: 0.656128\n",
      "no 5-fold loss: 0.6561282518074287\n",
      "MULTI WEIGHTED LOG LOSS: 0.65501\n",
      "CPU times: user 9min 55s, sys: 156 ms, total: 9min 56s\n",
      "Wall time: 37.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# modeling from CV\n",
    "clfs, score = eval_func(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to subm_0.655012_2018-12-05-16-03.csv\n"
     ]
    }
   ],
   "source": [
    "filename = 'subm_{:.6f}_{}.csv'.format(\n",
    "    score, \n",
    "    dt.now().strftime('%Y-%m-%d-%H-%M')\n",
    ")\n",
    "print('save to {}'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453653105 ../data/raw/test_set.csv\n"
     ]
    }
   ],
   "source": [
    "# calculate test size\n",
    "!wc -l ../data/raw/test_set.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4536532"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size_one_100 = int(453653105 / 100) + 1\n",
    "chunk_size_one_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:13<00:00,  2.17it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:05<00:00,  3.18it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:02<00:00,  7.02it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:02<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        4536532 done in   1.1 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:14<00:00,  1.55it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:05<00:00,  4.09it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:02<00:00,  6.87it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 12.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        9073064 done in   2.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:25<00:00,  1.02s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:06<00:00,  4.48it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:04<00:00,  4.66it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:03<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       13609596 done in   3.6 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:32<00:00,  1.40s/it]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:07<00:00,  3.02it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:05<00:00,  3.42it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:04<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       18146128 done in   5.6 minutes\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# should take 100x (time after 1st iteration)\n",
    "process_test(\n",
    "    clfs, \n",
    "    features=full_train.columns, \n",
    "    featurize_configs={'aggs': aggs, 'fcp': fcp}, \n",
    "    train_mean=train_mean, \n",
    "    filename=filename,\n",
    "    chunks=chunk_size_one_100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape BEFORE grouping: (3492890, 16)\n",
      "Shape AFTER grouping: (3492890, 15)\n"
     ]
    }
   ],
   "source": [
    "z = pd.read_csv(filename)\n",
    "print(\"Shape BEFORE grouping: {}\".format(z.shape))\n",
    "z = z.groupby('object_id').mean()\n",
    "print(\"Shape AFTER grouping: {}\".format(z.shape))\n",
    "z.to_csv('single_{}'.format(filename), index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
