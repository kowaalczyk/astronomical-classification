{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 68)\n"
     ]
    }
   ],
   "source": [
    "Xs = pd.read_csv('../data/sets/test-all-feat-from-kernel-repro.csv', nrows=1000000).drop('object_id', axis=1)\n",
    "Xs = np.nan_to_num(Xs.values)\n",
    "print(Xs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        assert input_size > latent_size + 2\n",
    "        mid_size = (input_size + latent_size) // 2 \n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, mid_size)\n",
    "        self.fc2mu = nn.Linear(mid_size, latent_size)\n",
    "        self.fc2var = nn.Linear(mid_size, latent_size)\n",
    "        self.fc3 = nn.Linear(latent_size, mid_size)\n",
    "        self.fc4 = nn.Linear(mid_size, input_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc2mu(h1), self.fc2var(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.01).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(Xs.shape[1], 2)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "reconstruction_function = nn.MSELoss(size_average=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    BCE = reconstruction_function(recon_x, x)  # mse loss\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    # KL divergence\n",
    "    print(f\"bce {BCE}, kld {KLD}\")\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std tensor([[2.6972e+01, 1.9488e-19],\n",
      "        [1.0656e+00, 2.0379e-01],\n",
      "        [9.1921e-01, 2.8432e-01],\n",
      "        ...,\n",
      "        [1.0210e+00, 6.0328e-01],\n",
      "        [1.1451e+00, 4.2066e-01],\n",
      "        [1.0526e+00, 4.8205e-01]], device='cuda:0', grad_fn=<ExpBackward>), lv tensor([[ 3.2948e+02, -4.3082e+03],\n",
      "        [ 6.3547e+00, -1.5907e+02],\n",
      "        [-8.4241e+00, -1.2576e+02],\n",
      "        ...,\n",
      "        [ 2.0780e+00, -5.0538e+01],\n",
      "        [ 1.3547e+01, -8.6593e+01],\n",
      "        [ 5.1238e+00, -7.2971e+01]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "z tensor([[ 4.2032e+02,  9.3114e+03],\n",
      "        [-2.0191e+00,  2.8988e+02],\n",
      "        [-6.2071e+00,  2.2190e+02],\n",
      "        ...,\n",
      "        [ 4.9586e+00,  7.6861e+01],\n",
      "        [ 5.5405e+00,  1.3463e+02],\n",
      "        [ 1.2187e+01,  1.4045e+02]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "bce nan, kld inf\n",
      "====> Epoch: 0 loss: nan\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X in [Xs]:\n",
    "        X = Variable(torch.tensor(X.astype(np.float32)))\n",
    "        if torch.cuda.is_available():\n",
    "            X = X.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(X)\n",
    "        loss = loss_function(recon_batch, X, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    #print(f'mu {mu}, logvar {logvar}')\n",
    "    print('====> Epoch: {} loss: {:.4f}'.format(\n",
    "        epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './vae.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
