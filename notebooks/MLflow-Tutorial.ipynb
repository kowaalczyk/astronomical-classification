{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow quickstart tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow is a tool to track machine learning experiments (and do a lot more things that we won't use):\n",
    "https://mlflow.org/docs/latest/quickstart.html  \n",
    "\n",
    "In this tutorial, we'll cover:\n",
    "1. Basic concepts of experiment tracking\n",
    "2. Setup processed used for this project\n",
    "3. Our extension of MLflow for experiments in this project\n",
    "  - `start_run` wrapper\n",
    "  - `log_args` helper\n",
    "4. How to define functions and what to log\n",
    "5. Sample experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic concepts\n",
    "\n",
    "As described on it's website:\n",
    "> MLflow Tracking is an API and UI for logging parameters, code versions, metrics, and output files when running your machine learning code and for later visualizing the results. You can use MLflow Tracking in any environment (for example, a standalone script or a notebook) to log results to local files or to a server, then compare multiple runs. Teams can also use it to compare results from different users.\n",
    "\n",
    "MLflow Tracking is organized around the concept of runs, which are executions of some piece of data science code. Each run records the following information and is a part of some experiment:\n",
    "\n",
    "> **Code Version**\n",
    "Git commit hash used to execute the run, if it was executed from an MLflow Project. We don't use MLflow project, but commit hash is automatically generated and logged from our custom wrapper for `start_run`.\n",
    "\n",
    "> **Start & End Time**\n",
    "Start and end time of the run.\n",
    "\n",
    "> **Source**\n",
    "Name of the file executed to launch the run, or the project name and entry point for the run if the run was executed from an MLflow Project.\n",
    "\n",
    "> **Parameters**\n",
    "Key-value input parameters of your choice. Both keys and values are strings.\n",
    "\n",
    "> **Metrics**\n",
    "Key-value metrics where the value is numeric. Each metric can be updated throughout the course of the run (for example, to track how your model’s loss function is converging), and MLflow will record and let you visualize the metric’s full history.\n",
    "\n",
    "> **Artifacts**\n",
    "Output files in any format. For example, you can record images (for example, PNGs), models (for example, a pickled scikit-learn model), or even data files (for example, a Parquet file) as artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Useful MLflow functions\n",
    "\n",
    "Managing experiments:\n",
    "\n",
    "> **mlflow.create_experiment()** creates a new experiment and returns its ID. Runs can be launched under the experiment by passing the experiment ID to mlflow.start_run.\n",
    "\n",
    "Managing runs:\n",
    "\n",
    "> **mlflow.start_run()** returns the currently active run (if one exists), or starts a new run and returns a mlflow.ActiveRun object usable as a context manager for the current run. You do not need to call start_run explicitly: calling one of the logging functions with no active run will automatically start a new one.\n",
    "\n",
    "> **mlflow.end_run()** ends the currently active run, if any, taking an optional run status.\n",
    "\n",
    "> **mlflow.active_run()** returns a mlflow.entities.Run object corresponding to the currently active run, if any.\n",
    "\n",
    "Logging experiments' inputs and results:\n",
    "\n",
    "> **mlflow.log_param()** logs a key-value parameter in the currently active run. The keys and values are both strings.\n",
    "\n",
    "> **mlflow.log_metric()** logs a key-value metric. The value must always be a number. MLflow will remember the history of values for each metric.\n",
    "\n",
    "> **mlflow.log_artifact()** logs a local file as an artifact, optionally taking an artifact_path to place it in within the run’s artifact URI. Run artifacts can be organized into directories, so you can place the artifact in a directory this way.\n",
    "\n",
    "> **mlflow.log_artifacts()** logs all the files in a given directory as artifacts, again taking an optional artifact_path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup used in our project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow is in requirements-dev.txt: `pip install -r ../requirements-dev.txt`.\n",
    "Our wrapper can be imported like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments import start_run, log_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use MLflow and jupyter, we have 2 tmux terminals running on the server constantly:\n",
    "- `lab` which runs `jupyter lab` on `localhost:8888`\n",
    "- `mlflow` which runs `mlflow ui` on `localhost:5000`\n",
    "\n",
    "To use both of these resources, you have to connect to ssh with two tunnels.\n",
    "\n",
    "MLflow configuration is default:\n",
    "- runs are saved to `mlruns` folder, it will be ignored by git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Experiments wrapper\n",
    "Our wrapper consists of 2 files: \n",
    "- `start_run` wrapper\n",
    "- `log_params` helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `experiments.start_run`\n",
    "- enforces run being started in a named experiment (by default, MLflow would create an unnamed one)\n",
    "- logs version information (git commit hash) that can be used to reproduce the project\n",
    "- provides a cleaner interface than `mlflow.start_run`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `experiments.log_params`\n",
    "- decorator for fucntions for easy logging of function parameters' values to mlflow\n",
    "- logs all passed params except ones that start with _underscore\n",
    "- does NOT support logging artifacts - this needs to be done manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Example - wrapper to define experiment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all examples are inspired by: \n",
    "# https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ParameterGrid\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_score_cv(colsample_bytree, n_estimators, max_depth, random_state):\n",
    "    sss = StratifiedShuffleSplit(n_splits=4, test_size=0.25, random_state=random_state)\n",
    "    scores = np.zeros(sss.get_n_splits(X, y))\n",
    "    for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n",
    "        # split dataset into train and test\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # build and train\n",
    "        model = XGBClassifier(\n",
    "            objective='multi:softmax',\n",
    "            num_class=14,\n",
    "            learning_rate=0.03,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_alpha=0.01,\n",
    "            reg_lambda=0.01,\n",
    "            min_child_weight=10,\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            nthread=1,\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        # predict and score\n",
    "        scores[i] = model.score(X_test, y_test)\n",
    "    mlflow.log_metric('mean_score', np.mean(scores))\n",
    "    mlflow.log_metric('std_score', np.std(scores))\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Example - running sample experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some sample data for the demonstration\n",
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1], [-1, -1], [-2, -1], [1, 1], [2, 1], [-1, -1], [-2, -1], [1, 1], [2, 1], [-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "y = np.array([1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our experiment tests model performance with all possible combinations of these parameters\n",
    "params_to_test = {\n",
    "    'colsample_bytree': [0.25, 0.5, 0.75],\n",
    "    'n_estimators': [2,8,32],\n",
    "    'max_depth': [2,3,4],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before starting runs, we need to create an experiment\n",
    "experiment_name = \"Demo - testing hyperparameters for SVC\"\n",
    "mlflow.create_experiment(name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n",
      "Mean score = 0.5\n"
     ]
    }
   ],
   "source": [
    "params_combinations = list(ParameterGrid(params_to_test))  # generate all combinations of parameters\n",
    "for params in params_combinations:\n",
    "    with start_run(experiment=experiment_name, source_name=\"MLflow-Tutorial\"):\n",
    "        # we use our custom wrapper here, not mlflow.start_run, to log commit hash and other important stuff automatically\n",
    "        mean_score = train_and_score_cv(random_state=2222, **params)\n",
    "        print(f\"Mean score = {mean_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run results are available in MLflow UI, at `localhost:5000`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
