{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ogólnie to nie działa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization & params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from functools import partial, wraps\n",
    "\n",
    "import plasticc.xgb_train as xgb\n",
    "import plasticc.lgbm_train as lgbm\n",
    "from plasticc.featurize import process_meta\n",
    "from plasticc.featurize import featurize\n",
    "from plasticc.lgbm_train import lgbm_modeling_cross_validation\n",
    "from plasticc.xgb_train import xgb_modeling_cross_validation\n",
    "from plasticc.final import featurize_test, predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Missing module name.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext plasticc.lgbm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcp = {\n",
    "    'flux': {\n",
    "        'longest_strike_above_mean': None,\n",
    "        'longest_strike_below_mean': None,\n",
    "        'mean_change': None,\n",
    "        'mean_abs_change': None,\n",
    "        'length': None,\n",
    "    },\n",
    "\n",
    "    'flux_by_flux_ratio_sq': {\n",
    "        'longest_strike_above_mean': None,\n",
    "        'longest_strike_below_mean': None,       \n",
    "    },\n",
    "\n",
    "    'flux_passband': {\n",
    "        'fft_coefficient': [\n",
    "                {'coeff': 0, 'attr': 'abs'}, \n",
    "                {'coeff': 1, 'attr': 'abs'}\n",
    "            ],\n",
    "        'kurtosis' : None, \n",
    "        'skewness' : None,\n",
    "    },\n",
    "\n",
    "    'mjd': {\n",
    "        'maximum': None, \n",
    "        'minimum': None,\n",
    "        'mean_change': None,\n",
    "        'mean_abs_change': None,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs = {\n",
    "    'flux': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "    'flux_err': ['min', 'max', 'mean', 'median', 'std', 'skew'],\n",
    "    'detected': ['mean'],\n",
    "    'flux_ratio_sq':['sum', 'skew'],\n",
    "    'flux_by_flux_ratio_sq':['sum','skew'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453653105 ../data/raw/test_set.csv\n"
     ]
    }
   ],
   "source": [
    "# calculate test size\n",
    "!wc -l ../data/raw/test_set.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4536532"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size_one_100 = 453653105 // 100 + 1\n",
    "#chunk_size_one_100 = 1000001 // 100 + 1\n",
    "chunk_size_one_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting features from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_featurized = True\n",
    "if not already_featurized:\n",
    "    featurize_test(\n",
    "        featurize_configs={'aggs': aggs, 'fcp': fcp}, \n",
    "        n_jobs=4,\n",
    "        meta_path='../data/raw/test_set_metadata.csv',\n",
    "        test_path='../data/raw/test_set.csv',\n",
    "       output_path='feat_test.csv',\n",
    "        id_colname='object_id',\n",
    "        chunks=5000000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1421705, 6)\n",
      "(7848, 15)\n",
      "CPU times: user 892 ms, sys: 880 ms, total: 1.77 s\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "meta_train = process_meta('../data/raw/training_set_metadata.csv')\n",
    "train = pd.read_csv('../data/raw/training_set.csv')\n",
    "print(train.shape)\n",
    "print(meta_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:11<00:00,  1.86it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:03<00:00,  6.21it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 16.03it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 13.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7848, 70)\n",
      "CPU times: user 26.2 s, sys: 8.44 s, total: 34.7 s\n",
      "Wall time: 42.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = featurize(train, meta_train, aggs, fcp, n_jobs=4)\n",
    "X_backup = X.copy()\n",
    "Xcols = X.columns\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7848, 68)\n"
     ]
    }
   ],
   "source": [
    "X = X_backup.copy()\n",
    "\n",
    "if 'target' in X:\n",
    "    y = X['target']\n",
    "    del X['target']\n",
    "else:\n",
    "    print(\"What the duck\")\n",
    "    3//0\n",
    "if 'object_id' in X:\n",
    "    del X['object_id']\n",
    "else:\n",
    "    print(\"What the heck\")\n",
    "    3//0\n",
    "print(X.shape)\n",
    "\n",
    "Xcols = X.columns\n",
    "\n",
    "X = X.fillna(0)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X = pd.DataFrame(data=X, columns=Xcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing featurized test set to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xinp = pd.read_csv('feat_test.csv').drop('object_id', axis=1)\n",
    "Xinp = np.nan_to_num(Xinp.values)\n",
    "Xinp = StandardScaler().fit_transform(Xinp)\n",
    "print(Xinp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting test set to batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "#batch_num = Xinp.shape[0]//batch_size\n",
    "#Xs = [Xinp[i*batch_size : (i+1)*batch_size] for i in range(0, batch_num)]\n",
    "batch_num = X.shape[0]//batch_size\n",
    "Xs = [X[i*batch_size : (i+1)*batch_size] for i in range(0, batch_num)]\n",
    "\n",
    "assert batch_num == len(Xs)\n",
    "assert all([s.shape[0] == batch_size for s in Xs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(autoencoder, self).__init__()\n",
    "\n",
    "        self.train_mode = True\n",
    "        \n",
    "        if input_size <= latent_size + 2:\n",
    "            raise Exception(\"input size is not enough bigger than latent size\")\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.latent_size = latent_size\n",
    "        self.mid_size = (input_size + latent_size) // 2 \n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, self.mid_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.mid_size, latent_size)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, self.mid_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.mid_size, input_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        if self.train_mode:\n",
    "            x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode_mode(self, b=False):\n",
    "        self.train_mode=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_savior = 2//0  # To not remove it accidentally\n",
    "\n",
    "model = autoencoder(input_size=Xs[0].shape[1],\n",
    "                    latent_size=30\n",
    "                   ).cuda(2)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate,\n",
    "                             weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training autoencoder and setting it to encoding mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/10], loss:0.8257\n",
      "epoch [2/10], loss:0.8202\n",
      "epoch [3/10], loss:0.8154\n",
      "epoch [4/10], loss:0.8111\n",
      "epoch [5/10], loss:0.8071\n",
      "epoch [6/10], loss:0.8034\n",
      "epoch [7/10], loss:0.7999\n",
      "epoch [8/10], loss:0.7965\n",
      "epoch [9/10], loss:0.7932\n",
      "epoch [10/10], loss:0.7901\n"
     ]
    }
   ],
   "source": [
    "#model_savior = 2//0  # To not remove it accidentally\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for X_ in Xs:\n",
    "        X_ = Variable(torch.tensor(X_.values.astype(np.float32))).cuda(2)\n",
    "        # ===================forward=====================\n",
    "        output = model(X_)\n",
    "        loss = criterion(output, X_)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    if epoch % (num_epochs//10) == 0 or epoch == num_epochs - 1:\n",
    "        print('epoch [{}/{}], loss:{:.4f}'\n",
    "              .format(epoch+1, num_epochs, loss.data.item()))\n",
    "model.eval()\n",
    "model.encode_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(X):\n",
    "    X = model(X)\n",
    "    X = pd.DataFrame(data=X.detach().cpu().numpy(), \n",
    "                 columns=['aut' + str(i) for i in range(0, X.shape[1])])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xv = Variable(torch.tensor(X.values.astype(np.float32))).cuda(2)\n",
    "X = X.join(runModel(Xv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes : 14, [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
      "{6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n"
     ]
    }
   ],
   "source": [
    "# Taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "# https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "# with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "classes = sorted(y.unique())\n",
    "class_weights = {c: 1 for c in classes}\n",
    "class_weights.update({c:2 for c in [64, 15]})\n",
    "print('Unique classes : {}, {}'.format(len(classes), classes))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {'max_depth': 8,\n",
    " 'n_estimators': 1024,\n",
    " 'max_bin': 16,\n",
    " 'num_leaves': 5,\n",
    " 'feature_fraction': 0.5,\n",
    " 'verbosity': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_func = partial(\n",
    "    lgbm_modeling_cross_validation, \n",
    "    X=X, \n",
    "    y=y, \n",
    "    classes=classes, \n",
    "    class_weights=class_weights, \n",
    "    nr_fold=5, \n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.441699\ttraining's wloss: 0.430973\tvalid_1's multi_logloss: 0.817372\tvalid_1's wloss: 0.671012\n",
      "Early stopping, best iteration is:\n",
      "[118]\ttraining's multi_logloss: 0.407722\ttraining's wloss: 0.396876\tvalid_1's multi_logloss: 0.796825\tvalid_1's wloss: 0.667118\n",
      "no 1-fold loss: 0.6671183407500729\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.445121\ttraining's wloss: 0.435444\tvalid_1's multi_logloss: 0.820503\tvalid_1's wloss: 0.726302\n",
      "Early stopping, best iteration is:\n",
      "[120]\ttraining's multi_logloss: 0.406731\ttraining's wloss: 0.396353\tvalid_1's multi_logloss: 0.795336\tvalid_1's wloss: 0.722553\n",
      "no 2-fold loss: 0.7225532219655024\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.446292\ttraining's wloss: 0.437064\tvalid_1's multi_logloss: 0.80661\tvalid_1's wloss: 0.614217\n",
      "[200]\ttraining's multi_logloss: 0.310676\ttraining's wloss: 0.300136\tvalid_1's multi_logloss: 0.716485\tvalid_1's wloss: 0.577829\n",
      "Early stopping, best iteration is:\n",
      "[224]\ttraining's multi_logloss: 0.289447\ttraining's wloss: 0.278992\tvalid_1's multi_logloss: 0.702957\tvalid_1's wloss: 0.575556\n",
      "no 3-fold loss: 0.5755559490998697\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.446166\ttraining's wloss: 0.43639\tvalid_1's multi_logloss: 0.795919\tvalid_1's wloss: 0.650479\n",
      "Early stopping, best iteration is:\n",
      "[122]\ttraining's multi_logloss: 0.405166\ttraining's wloss: 0.394778\tvalid_1's multi_logloss: 0.767797\tvalid_1's wloss: 0.643087\n",
      "no 4-fold loss: 0.6430866679152688\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.440304\ttraining's wloss: 0.430026\tvalid_1's multi_logloss: 0.793618\tvalid_1's wloss: 0.68026\n",
      "[200]\ttraining's multi_logloss: 0.307183\ttraining's wloss: 0.295823\tvalid_1's multi_logloss: 0.698928\tvalid_1's wloss: 0.65803\n",
      "Early stopping, best iteration is:\n",
      "[167]\ttraining's multi_logloss: 0.340117\ttraining's wloss: 0.328835\tvalid_1's multi_logloss: 0.719596\tvalid_1's wloss: 0.654574\n",
      "no 5-fold loss: 0.6545744523526644\n",
      "MULTI WEIGHTED LOG LOSS: 0.65242\n",
      "CPU times: user 3min 43s, sys: 672 ms, total: 3min 43s\n",
      "Wall time: 18.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# modeling from CV\n",
    "clfs, score = eval_func(lgbm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'subm_{:.6f}_{}.csv'.format(\n",
    "    score, \n",
    "    dt.now().strftime('%Y-%m-%d-%H-%M')\n",
    ")\n",
    "print('save to {}'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# should take 100x (time after 1st iteration)\n",
    "predict_test(\n",
    "    clfs,\n",
    "    features=X.columns, \n",
    "    output_path=filename,\n",
    "    input_path='feat_test.csv',\n",
    "    chunks=chunk_size_one_100,\n",
    "    n_jobs=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.read_csv(filename)\n",
    "print(\"Shape BEFORE grouping: {}\".format(z.shape))\n",
    "z = z.groupby('object_id').mean()\n",
    "print(\"Shape AFTER grouping: {}\".format(z.shape))\n",
    "z.to_csv('single_{}'.format(filename), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
